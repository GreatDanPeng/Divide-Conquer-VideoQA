{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.config import Config\n",
    "config_file = \"configs/config.json\"\n",
    "cfg = Config.from_file(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models.videochat import VideoChat\n",
    "from utils.easydict import EasyDict\n",
    "import torch\n",
    "\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from decord import VideoReader, cpu\n",
    "import torchvision.transforms as T\n",
    "from models.video_transformers import (\n",
    "    GroupNormalize, GroupScale, GroupCenterCrop, \n",
    "    Stack, ToTorchFormatTensor\n",
    ")\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'google-bert/bert-base-uncased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'google-bert/bert-base-uncased' is the correct path to a directory containing all relevant files for a BertTokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mVideoChat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/autodl-tmp/Ask-Anything/video_chat/models/videochat.py:48\u001b[0m, in \u001b[0;36mVideoChat.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     45\u001b[0m num_query_token \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_query_token\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m extra_num_query_token \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_num_query_token\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_resource \u001b[38;5;241m=\u001b[39m low_resource\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvit_precision \u001b[38;5;241m=\u001b[39m vit_precision\n",
      "File \u001b[0;32m~/autodl-tmp/Ask-Anything/video_chat/models/blip2.py:25\u001b[0m, in \u001b[0;36mBlip2Base.init_tokenizer\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_tokenizer\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n\u001b[0;32m---> 25\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBertTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle-bert/bert-base-uncased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39madd_special_tokens({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbos_token\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[DEC]\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2197\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[1;32m   2196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[0;32m-> 2197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2198\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2199\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2202\u001b[0m     )\n\u001b[1;32m   2204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'google-bert/bert-base-uncased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'google-bert/bert-base-uncased' is the correct path to a directory containing all relevant files for a BertTokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "model = VideoChat(config=cfg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.to(torch.device(cfg.device))\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(conv):\n",
    "    ret = conv.system + conv.sep\n",
    "    for role, message in conv.messages:\n",
    "        if message:\n",
    "            ret += role + \": \" + message + conv.sep\n",
    "        else:\n",
    "            ret += role + \":\"\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_context_emb(conv, model, img_list):\n",
    "    prompt = get_prompt(conv)\n",
    "    print(prompt)\n",
    "    if '<VideoHere>' in prompt:\n",
    "        prompt_segs = prompt.split('<VideoHere>')\n",
    "    else:\n",
    "        prompt_segs = prompt.split('<ImageHere>')\n",
    "    assert len(prompt_segs) == len(img_list) + 1, \"Unmatched numbers of image placeholders and images.\"\n",
    "    seg_tokens = [\n",
    "        model.llama_tokenizer(\n",
    "            seg, return_tensors=\"pt\", add_special_tokens=i == 0).to(\"cuda:0\").input_ids\n",
    "        # only add bos to the first seg\n",
    "        for i, seg in enumerate(prompt_segs)\n",
    "    ]\n",
    "    seg_embs = [model.llama_model.model.embed_tokens(seg_t) for seg_t in seg_tokens]\n",
    "    mixed_embs = [emb for pair in zip(seg_embs[:-1], img_list) for emb in pair] + [seg_embs[-1]]\n",
    "    mixed_embs = torch.cat(mixed_embs, dim=1)\n",
    "    return mixed_embs\n",
    "\n",
    "\n",
    "def ask(text, conv):\n",
    "    conv.messages.append([conv.roles[0], text + '\\n'])\n",
    "        \n",
    "\n",
    "class StoppingCriteriaSub(StoppingCriteria):\n",
    "    def __init__(self, stops=[], encounters=1):\n",
    "        super().__init__()\n",
    "        self.stops = stops\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n",
    "        for stop in self.stops:\n",
    "            if torch.all((stop == input_ids[0][-len(stop):])).item():\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "def answer(conv, model, img_list, max_new_tokens=200, num_beams=1, min_length=1, top_p=0.9,\n",
    "               repetition_penalty=1.0, length_penalty=1, temperature=1.0):\n",
    "    stop_words_ids = [\n",
    "        torch.tensor([835]).to(\"cuda:0\"),\n",
    "        torch.tensor([2277, 29937]).to(\"cuda:0\")]  # '###' can be encoded in two different ways.\n",
    "    stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n",
    "    \n",
    "    conv.messages.append([conv.roles[1], None])\n",
    "    embs = get_context_emb(conv, model, img_list)\n",
    "    outputs = model.llama_model.generate(\n",
    "        inputs_embeds=embs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        stopping_criteria=stopping_criteria,\n",
    "        num_beams=num_beams,\n",
    "        do_sample=True,\n",
    "        min_length=min_length,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        length_penalty=length_penalty,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    output_token = outputs[0]\n",
    "    if output_token[0] == 0:  # the model might output a unknow token <unk> at the beginning. remove it\n",
    "            output_token = output_token[1:]\n",
    "    if output_token[0] == 1:  # some users find that there is a start token <s> at the beginning. remove it\n",
    "            output_token = output_token[1:]\n",
    "    output_text = model.llama_tokenizer.decode(output_token, add_special_tokens=False)\n",
    "    output_text = output_text.split('###')[0]  # remove the stop sign '###'\n",
    "    output_text = output_text.split('Assistant:')[-1].strip()\n",
    "    conv.messages[-1][1] = output_text\n",
    "    return output_text, output_token.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(num_frames, num_segments):\n",
    "    seg_size = float(num_frames - 1) / num_segments\n",
    "    start = int(seg_size / 2)\n",
    "    offsets = np.array([\n",
    "        start + int(np.round(seg_size * idx)) for idx in range(num_segments)\n",
    "    ])\n",
    "    return offsets\n",
    "\n",
    "\n",
    "def load_video(video_path, num_segments=8, return_msg=False):\n",
    "    vr = VideoReader(video_path, ctx=cpu(0))\n",
    "    num_frames = len(vr)\n",
    "    frame_indices = get_index(num_frames, num_segments)\n",
    "\n",
    "    # transform\n",
    "    crop_size = 224\n",
    "    scale_size = 224\n",
    "    input_mean = [0.48145466, 0.4578275, 0.40821073]\n",
    "    input_std = [0.26862954, 0.26130258, 0.27577711]\n",
    "\n",
    "    transform = T.Compose([\n",
    "        GroupScale(int(scale_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        GroupCenterCrop(crop_size),\n",
    "        Stack(),\n",
    "        ToTorchFormatTensor(),\n",
    "        GroupNormalize(input_mean, input_std) \n",
    "    ])\n",
    "\n",
    "    images_group = list()\n",
    "    for frame_index in frame_indices:\n",
    "        img = Image.fromarray(vr[frame_index].asnumpy())\n",
    "        images_group.append(img)\n",
    "    torch_imgs = transform(images_group)\n",
    "    if return_msg:\n",
    "        fps = float(vr.get_avg_fps())\n",
    "        sec = \", \".join([str(round(f / fps, 1)) for f in frame_indices])\n",
    "        # \" \" should be added in the start and end\n",
    "        msg = f\"The video contains {len(frame_indices)} frames sampled at {sec} seconds.\"\n",
    "        return torch_imgs, msg\n",
    "    else:\n",
    "        return torch_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video contains 8 frames sampled at 0.6, 1.8, 3.0, 4.2, 5.5, 6.7, 7.9, 9.1 seconds.\n"
     ]
    }
   ],
   "source": [
    "# vid_path = \"./example/yoga.mp4\"\n",
    "vid_path = \"./example/jesse_dance.mp4\"\n",
    "vid, msg = load_video(vid_path, num_segments=8, return_msg=True)\n",
    "print(msg)\n",
    "    \n",
    "# The model expects inputs of shape: T x C x H x W\n",
    "TC, H, W = vid.shape\n",
    "video = vid.reshape(1, TC//3, 3, H, W).to(\"cuda:0\")\n",
    "\n",
    "img_list = []\n",
    "image_emb, _ = model.encode_img(video)\n",
    "img_list.append(image_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###Human: <Video><VideoHere></Video> The video contains 8 frames sampled at 0.6, 1.8, 3.0, 4.2, 5.5, 6.7, 7.9, 9.1 seconds.\n",
      "###Human: Explain why it is funny?\n",
      "###Assistant:\n",
      "This is a funny video of a man dancing in a kitchen with his handy family of paper people. The man is clearly having fun, and the fact that the paper people are also moving and dancing alongside him adds to the humor of the video. The man's exuberance and enthusiasm while dancing with the paper people makes the video particularly entertaining. It is a lighthearted and whimsical moment that showcases a man's playful side. The use of paper people adds a touch of creativity and imagination to the video. Overall, it is a funny and heartwarming video.\n"
     ]
    }
   ],
   "source": [
    "chat = EasyDict({\n",
    "#     \"system\": \"You are an AI assistant. A human gives an image or a video and asks some questions. You should give helpful, detailed, and polite answers.\\n\",\n",
    "    \"system\": \"\",\n",
    "    \"roles\": (\"Human\", \"Assistant\"),\n",
    "    \"messages\": [],\n",
    "    \"sep\": \"###\"\n",
    "})\n",
    "\n",
    "chat.messages.append([chat.roles[0], f\"<Video><VideoHere></Video> {msg}\\n\"])\n",
    "# ask(\"Describe the video in detail.\", chat)\n",
    "# ask(\"Is she safe to doing something in the video?\", chat)\n",
    "# ask(\"Where is she?\", chat)\n",
    "# ask(\"Who are you?\", chat)\n",
    "# ask(\"Are you an assistant?\", chat)\n",
    "# ask(\"How can I learn to do as the video?\", chat)\n",
    "# ask(\"Tell me what she did at the fourth second\", chat)\n",
    "# ask(\"Can you provide me some urls to learn to do as the video?\", chat)\n",
    "# ask(\"List the president of America.\", chat)\n",
    "# ask(\"What do you feel from the video?\", chat)\n",
    "# ask(\"Do you think it is funny? Why?\", chat)\n",
    "ask(\"Explain why it is funny?\", chat)\n",
    "# ask(\"Explain why the video is ridiculous?\", chat)\n",
    "\n",
    "llm_message = answer(conv=chat, model=model, img_list=img_list, max_new_tokens=1000)[0]\n",
    "print(llm_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_path = \"./example/bear.jpg\"\n",
    "# img_path = \"./example/cxk.png\"\n",
    "img_path = \"./example/dog.png\"\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(\n",
    "            (224, 224), interpolation=InterpolationMode.BICUBIC\n",
    "        ),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "img = transform(img).unsqueeze(0).unsqueeze(0).cuda()\n",
    "img_list = []\n",
    "image_emb, _ = model.encode_img(img)\n",
    "img_list.append(image_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###Human: <Image><ImageHere></Image>\n",
      "###Human: Explain why the image is funny?\n",
      "###Assistant:\n",
      "The image is funny because the puppy is sleeping on a wooden floor with the words, \"just monday\" written underneath it. This implies that the puppy is lazy and is using this as an excuse for not having to do anything else on Monday. The words, \"just monday,\" are written in a bold font, emphasizing the puppy 's laziness and disdain for Monday. The puppy is lying down on the floor, which adds to the humor and implies that it doesn't want to move or do anything on this day of the week. The photo is taken from a slightly upward angle, which adds to the visual impact and creates a comical effect. The caption is written in a playful, humorous font that reinforces the joke and encourages viewers to have a good laugh. The combination of the puppy, the words, and the photo make this image funny and entertaining.\n"
     ]
    }
   ],
   "source": [
    "chat = EasyDict({\n",
    "#     \"system\": \"You are an AI assistant. A human gives an Image and asks questions. You should give helpful, detailed, and polite answers.\\n\",\n",
    "    \"system\": \"\",\n",
    "    \"roles\": (\"Human\", \"Assistant\"),\n",
    "    \"messages\": [],\n",
    "    \"sep\": \"###\"\n",
    "})\n",
    "\n",
    "chat.messages.append([chat.roles[0], \"<Image><ImageHere></Image>\\n\"])\n",
    "# ask(\"Describe the image in detail.\", chat)\n",
    "# ask(\"What's in the image?\", chat)\n",
    "# ask(\"Who are you?\", chat)\n",
    "# ask(\"Are you an assistant?\", chat) \n",
    "# ask(\"Where can I find the animal?\", chat)\n",
    "# ask(\"Give me some urls to find the anmimal\", chat)\n",
    "# ask(\"Give me some urls to learn to do it\", chat)\n",
    "# ask(\"Is it funny? why?\", chat)\n",
    "# ask(\"What do you feel from the image?\", chat)\n",
    "# ask(\"Do you think the image is funny?\", chat)\n",
    "ask(\"Explain why the image is funny?\", chat)\n",
    "# ask(\"What is the text in the image?\", chat)\n",
    "\n",
    "llm_message = answer(conv=chat, model=model, img_list=img_list, max_new_tokens=1000)[0]\n",
    "print(llm_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
